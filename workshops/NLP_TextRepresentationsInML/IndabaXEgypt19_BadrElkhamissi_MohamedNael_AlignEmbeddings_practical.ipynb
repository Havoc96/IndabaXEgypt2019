{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of AlignEmbeddings.ipynb","version":"0.3.2","provenance":[{"file_id":"1PvJdoJm_FNhURBnIpa197UWrHvyHOBZE","timestamp":1558463183102}]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Rtw65pQ5_M2N","colab_type":"text"},"source":["## Cross Lingual Word Embeddings\n","\n","** This Notebook is Adapted from [```fastText_multilingual``` by Babylon Partners](\"https://github.com/Babylonpartners/fastText_multilingual\")\n"]},{"cell_type":"markdown","metadata":{"id":"AQPsMotF_3g-","colab_type":"text"},"source":["**First Download Word Embeddings**"]},{"cell_type":"code","metadata":{"id":"AICoYYEO7jnY","colab_type":"code","colab":{}},"source":["!wget https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.fr.vec\n","!wget https://dl.fbaipublicfiles.com/fasttext/vectors-wiki/wiki.en.vec"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7xE5CaQP-Sj7","colab_type":"code","cellView":"form","colab":{}},"source":["#@title FastVector Class\n","#\n","# Copyright (c) 2017-present, babylon health\n","# All rights reserved.\n","#\n","# This source code is licensed under the BSD-style license found in the\n","# LICENSE file in the root directory of this source tree.\n","#\n","\n","import numpy as np\n","\n","\n","class FastVector:\n","    \"\"\"\n","    Minimal wrapper for fastvector embeddings.\n","    ```\n","    Usage:\n","        $ model = FastVector(vector_file='/path/to/wiki.en.vec')\n","        $ 'apple' in model\n","        > TRUE\n","        $ model['apple'].shape\n","        > (300,)\n","    ```\n","    \"\"\"\n","\n","    def __init__(self, vector_file='', transform=None):\n","        \"\"\"Read in word vectors in fasttext format\"\"\"\n","        self.word2id = {}\n","\n","        # Captures word order, for export() and translate methods\n","        self.id2word = []\n","        \n","        limit = 5000\n","\n","        print('reading word vectors from %s' % vector_file)\n","        with open(vector_file, 'r') as f:\n","            (self.n_words, self.n_dim) = \\\n","                (int(x) for x in f.readline().rstrip('\\n').split(' '))\n","            self.embed = np.zeros((self.n_words, self.n_dim))\n","            for i, line in enumerate(f):\n","                elems = line.rstrip('\\n').split(' ')\n","                self.word2id[elems[0]] = i\n","                self.embed[i] = elems[1:self.n_dim+1]\n","                self.id2word.append(elems[0])\n","                if i > limit: break\n","        \n","        # Used in translate_inverted_softmax()\n","        self.softmax_denominators = None\n","        \n","        if transform is not None:\n","            print('Applying transformation to embedding')\n","            self.apply_transform(transform)\n","\n","    def apply_transform(self, transform):\n","        \"\"\"\n","        Apply the given transformation to the vector space\n","\n","        Right-multiplies given transform with embeddings E:\n","            E = E * transform\n","\n","        Transform can either be a string with a filename to a\n","        text file containing a ndarray (compat. with np.loadtxt)\n","        or a numpy ndarray.\n","        \"\"\"\n","        transmat = np.loadtxt(transform) if isinstance(transform, str) else transform\n","        self.embed = np.matmul(self.embed, transmat)\n","\n","    def export(self, outpath):\n","        \"\"\"\n","        Transforming a large matrix of WordVectors is expensive. \n","        This method lets you write the transformed matrix back to a file for future use\n","        :param The path to the output file to be written \n","        \"\"\"\n","        fout = open(outpath, \"w\")\n","\n","        # Header takes the guesswork out of loading by recording how many lines, vector dims\n","        fout.write(str(self.n_words) + \" \" + str(self.n_dim) + \"\\n\")\n","        for token in self.id2word:\n","            vector_components = [\"%.6f\" % number for number in self[token]]\n","            vector_as_string = \" \".join(vector_components)\n","\n","            out_line = token + \" \" + vector_as_string + \"\\n\"\n","            fout.write(out_line)\n","\n","        fout.close()\n","\n","    def translate_nearest_neighbour(self, source_vector):\n","        \"\"\"Obtain translation of source_vector using nearest neighbour retrieval\"\"\"\n","        similarity_vector = np.matmul(FastVector.normalised(self.embed), source_vector)\n","        target_id = np.argmax(similarity_vector)\n","        return self.id2word[target_id]\n","\n","    def translate_inverted_softmax(self, source_vector, source_space, nsamples,\n","                                   beta=10., batch_size=100, recalculate=True):\n","        \"\"\"\n","        Obtain translation of source_vector using sampled inverted softmax retrieval\n","        with inverse temperature beta.\n","\n","        nsamples vectors are drawn from source_space in batches of batch_size\n","        to calculate the inverted softmax denominators.\n","        Denominators from previous call are reused if recalculate=False. This saves\n","        time if multiple words are translated from the same source language.\n","        \"\"\"\n","        embed_normalised = FastVector.normalised(self.embed)\n","        # calculate contributions to softmax denominators in batches\n","        # to save memory\n","        if self.softmax_denominators is None or recalculate is True:\n","            self.softmax_denominators = np.zeros(self.embed.shape[0])\n","            while nsamples > 0:\n","                # get batch of randomly sampled vectors from source space\n","                sample_vectors = source_space.get_samples(min(nsamples, batch_size))\n","                # calculate cosine similarities between sampled vectors and\n","                # all vectors in the target space\n","                sample_similarities = \\\n","                    np.matmul(embed_normalised,\n","                              FastVector.normalised(sample_vectors).transpose())\n","                # accumulate contribution to denominators\n","                self.softmax_denominators \\\n","                    += np.sum(np.exp(beta * sample_similarities), axis=1)\n","                nsamples -= batch_size\n","        # cosine similarities between source_vector and all target vectors\n","        similarity_vector = np.matmul(embed_normalised,\n","                                      source_vector/np.linalg.norm(source_vector))\n","        # exponentiate and normalise with denominators to obtain inverted softmax\n","        softmax_scores = np.exp(beta * similarity_vector) / \\\n","                         self.softmax_denominators\n","        # pick highest score as translation\n","        target_id = np.argmax(softmax_scores)\n","        return self.id2word[target_id]\n","\n","    def get_samples(self, nsamples):\n","        \"\"\"Return a matrix of nsamples randomly sampled vectors from embed\"\"\"\n","        sample_ids = np.random.choice(self.embed.shape[0], nsamples, replace=False)\n","        return self.embed[sample_ids]\n","\n","    @classmethod\n","    def normalised(cls, mat, axis=-1, order=2):\n","        \"\"\"Utility function to normalise the rows of a numpy array.\"\"\"\n","        norm = np.linalg.norm(\n","            mat, axis=axis, ord=order, keepdims=True)\n","        norm[norm == 0] = 1\n","        return mat / norm\n","    \n","    @classmethod\n","    def cosine_similarity(cls, vec_a, vec_b):\n","        \"\"\"Compute cosine similarity between vec_a and vec_b\"\"\"\n","        return np.dot(vec_a, vec_b) / \\\n","            (np.linalg.norm(vec_a) * np.linalg.norm(vec_b))\n","\n","    def __contains__(self, key):\n","        return key in self.word2id\n","\n","    def __getitem__(self, key):\n","        return self.embed[self.word2id[key]]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lox_ubAVAZuv","colab_type":"text"},"source":["**Let's define some utility functions**"]},{"cell_type":"code","metadata":{"id":"QGSKoDLD8E33","colab_type":"code","colab":{}},"source":["import numpy as np\n","\n","# from https://stackoverflow.com/questions/21030391/how-to-normalize-array-numpy\n","def normalized(a, axis=-1, order=2):\n","    \"\"\"Utility function to normalize the rows of a numpy array.\"\"\"\n","    l2 = np.atleast_1d(np.linalg.norm(a, order, axis))\n","    l2[l2==0] = 1\n","    return a / np.expand_dims(l2, axis)\n","\n","def make_training_matrices(source_dictionary, target_dictionary, bilingual_dictionary):\n","    \"\"\"\n","    Source and target dictionaries are the FastVector objects of\n","    source/target languages. bilingual_dictionary is a list of \n","    translation pair tuples [(source_word, target_word), ...].\n","    \"\"\"\n","    source_matrix = []\n","    target_matrix = []\n","\n","    for (source, target) in bilingual_dictionary:\n","        if source in source_dictionary and target in target_dictionary:\n","            source_matrix.append(source_dictionary[source])\n","            target_matrix.append(target_dictionary[target])\n","\n","    # return training matrices\n","    return np.array(source_matrix), np.array(target_matrix)\n","\n","def learn_transformation(source_matrix, target_matrix, normalize_vectors=True):\n","    \"\"\"\n","    Source and target matrices are numpy arrays, shape\n","    (dictionary_length, embedding_dimension). These contain paired\n","    word vectors from the bilingual dictionary.\n","    \"\"\"\n","    # optionally normalize the training vectors\n","    if normalize_vectors:\n","        source_matrix = normalized(source_matrix)\n","        target_matrix = normalized(target_matrix)\n","\n","    # perform the SVD\n","    product = np.matmul(source_matrix.transpose(), target_matrix)\n","    U, s, V = np.linalg.svd(product)\n","\n","    # return orthogonal transformation which aligns source language to the target\n","    return np.matmul(U, V)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rnMgL2LRARVQ","colab_type":"text"},"source":["Now load French and English word embeddings then compute similarity of \"chat\" and \"cat\""]},{"cell_type":"code","metadata":{"id":"gSQprwcE8QR3","colab_type":"code","outputId":"b75499ad-f590-4326-dade-0adc92ef7000","executionInfo":{"status":"ok","timestamp":1556983120781,"user_tz":-120,"elapsed":7537,"user":{"displayName":"Badr AlKhamissi","photoUrl":"https://lh3.googleusercontent.com/-46ky1CDDoe4/AAAAAAAAAAI/AAAAAAAAAG8/FRJEhoDkAsE/s64/photo.jpg","userId":"00092854506256827387"}},"colab":{"base_uri":"https://localhost:8080/","height":68}},"source":["fr_dictionary = FastVector(vector_file='wiki.fr.vec')\n","en_dictionary = FastVector(vector_file='wiki.en.vec')\n","\n","fr_vector = fr_dictionary[\"chat\"]\n","en_vector = en_dictionary[\"cat\"]\n","print(FastVector.cosine_similarity(fr_vector, en_vector))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["reading word vectors from wiki.fr.vec\n","reading word vectors from wiki.en.vec\n","-0.0643938119394162\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dNVs48NRAzUx","colab_type":"text"},"source":["Form bilingual dictionary by extracting identical character strings"]},{"cell_type":"code","metadata":{"id":"6KTEIyOk8z1m","colab_type":"code","outputId":"b409570d-62c8-434e-c654-3fd87e52ee84","executionInfo":{"status":"ok","timestamp":1556983120783,"user_tz":-120,"elapsed":3855,"user":{"displayName":"Badr AlKhamissi","photoUrl":"https://lh3.googleusercontent.com/-46ky1CDDoe4/AAAAAAAAAAI/AAAAAAAAAG8/FRJEhoDkAsE/s64/photo.jpg","userId":"00092854506256827387"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["en_words = set(en_dictionary.word2id.keys())\n","fr_words = set(fr_dictionary.word2id.keys())\n","overlap = list(en_words & fr_words)\n","bilingual_dictionary = [(entry, entry) for entry in overlap]\n","\n","len(bilingual_dictionary)"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1109"]},"metadata":{"tags":[]},"execution_count":4}]},{"cell_type":"markdown","metadata":{"id":"4Iz3AKTBBHNv","colab_type":"text"},"source":["Learn transformation matrix"]},{"cell_type":"code","metadata":{"id":"Vn56-DhM87oY","colab_type":"code","colab":{}},"source":["# form the training matrices\n","source_matrix, target_matrix = make_training_matrices(\n","    fr_dictionary, en_dictionary, bilingual_dictionary)\n","\n","# learn and apply the transformation\n","transform = learn_transformation(source_matrix, target_matrix)\n","fr_dictionary.apply_transform(transform)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MwqXt9ynBKMd","colab_type":"text"},"source":["Compute similarity after transformation "]},{"cell_type":"code","metadata":{"id":"SwVSo8Ou9Aps","colab_type":"code","outputId":"2b226059-b9aa-4c49-af31-0c2e44ca417c","executionInfo":{"status":"ok","timestamp":1556983142999,"user_tz":-120,"elapsed":1015,"user":{"displayName":"Badr AlKhamissi","photoUrl":"https://lh3.googleusercontent.com/-46ky1CDDoe4/AAAAAAAAAAI/AAAAAAAAAG8/FRJEhoDkAsE/s64/photo.jpg","userId":"00092854506256827387"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["fr_vector = fr_dictionary[\"chat\"]\n","en_vector = en_dictionary[\"cat\"]\n","print(FastVector.cosine_similarity(fr_vector, en_vector))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["0.48891059694919403\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"BY4GD0XQHvWM","colab_type":"text"},"source":["**Translate using Nearest Neighbours **"]},{"cell_type":"code","metadata":{"id":"jMDSEIcjHuqY","colab_type":"code","outputId":"e41576d4-7df2-4c9c-c0a0-850878d5f1a3","executionInfo":{"status":"ok","timestamp":1556983426445,"user_tz":-120,"elapsed":3712,"user":{"displayName":"Badr AlKhamissi","photoUrl":"https://lh3.googleusercontent.com/-46ky1CDDoe4/AAAAAAAAAAI/AAAAAAAAAG8/FRJEhoDkAsE/s64/photo.jpg","userId":"00092854506256827387"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["word = 'dog'\n","translation = fr_dictionary.translate_nearest_neighbour(en_dictionary[word])\n","print(f\"Translation of {word} --> {translation}\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Translation of dog --> chien\n"],"name":"stdout"}]}]}