{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of IndabaXEgypt-ReinforcementLearning-Practical.ipynb","version":"0.3.2","provenance":[{"file_id":"1JV8-PAB5m_YJ3YfJxSLHH5X43vcU9Fok","timestamp":1558463135636}],"collapsed_sections":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"2j01wZ6zYyHe","colab_type":"text"},"source":["# Time to Practice ...\n","\n","After going through background content about the of Reinforcement Learning and its taxonomy, here comes the time to practice. In this notebook, we are going to explore multiple problems and practice solving them using one the most popular RL algorithms, Q-Learning. Along this practical we are going to go through\n","1. a very simple implementation of Q-Learning where we will be required to implement the Q function.\n","2. next, we will apply few enhancements to our implementation by handling the exploration-exploitation issue as well as exploring the effect of the different hyperparameter values on agent training. \n","3. Finally, we are going to implement the Q function approximator to run the Deep Q Network model.\n","\n","\n","#### Objectives\n","- experiment with some basic to advanced environments in OpenAI gym\n","- implement a simple Q function for the case of discre and continuous spaces\n","- implement epislon-greedy exploration algorithm with Q learning\n","- experiment with Q function approximation with DQN on Atari games\n","\n","Finally, ask your tutors if you needed any help!\n","\n","\n","### Let's start ....\n","\n"]},{"cell_type":"markdown","metadata":{"id":"CyFfvGQ6YpMW","colab_type":"text"},"source":["## First Exercise : Taxi Game\n","\n","Taxi-v2 is an example of toy text environments. Where there are 4 locations, labelled by different letters, and our job is to pick up the passenger at one location and drop him off at another. We receive +20 points for a successful drop-off and lose 1 point for every time-step it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions.\n","\n","\n","The taxi is the only car in this parking lot. *italicized text*\n","We can break up the parking lot into a 5x5 grid, which gives us 25 possible taxi locations. \n","These 25 locations are one part of our state space. \n","\n","In the environment, there are four possible locations where you can drop the passengers in the taxi which are: R, G, Y, B or [(0,0), (0,4), (4,0), (4,3)] in (row, col) coordinates \n","\n","When we also account for one (1) additional passenger state of being inside the taxi, \n","we can take all combinations of passenger locations and destination locations to come to a total number of states for our taxi environment; \n","there are four (4) destinations and five (4 + 1) passenger locations. \n","\n","So, our taxi environment has 5×5×5×4=500 total possible states. The agent encounters one of the 500 states, and it takes action. \n","\n","We have six possible actions: pickup, drop, north, east, south, west(These four directions are the moves by which the taxi is moved.)\n","\n","\n","#### Let's start practice ..\n","Import gym and load the environment."]},{"cell_type":"code","metadata":{"id":"imfFDO4YYpMZ","colab_type":"code","outputId":"a26f0f65-4ce4-4bc0-ef32-dd09f7496538","executionInfo":{"status":"ok","timestamp":1556969236541,"user_tz":-120,"elapsed":1283,"user":{"displayName":"Amal Abdelsalam","photoUrl":"https://lh6.googleusercontent.com/-MsHyBKvIoWc/AAAAAAAAAAI/AAAAAAAAAGk/D_9qsqnwXZE/s64/photo.jpg","userId":"16383286328372397172"}},"colab":{"base_uri":"https://localhost:8080/","height":156}},"source":["import gym\n","import numpy as np\n","from IPython.display import clear_output\n","\n","# Init Taxi-V2 Env\n","env = gym.make(\"Taxi-v2\").env\n","env.render()"],"execution_count":0,"outputs":[{"output_type":"stream","text":["+---------+\n","|R: | : :G|\n","| : : : : |\n","| : : : : |\n","| | : | : |\n","|\u001b[35m\u001b[43mY\u001b[0m\u001b[0m| : |\u001b[34;1mB\u001b[0m: |\n","+---------+\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"AEvziIJeYpMk","colab_type":"text"},"source":["#### Let's check how does the state look like?"]},{"cell_type":"code","metadata":{"id":"4zc7PxX4YpMm","colab_type":"code","outputId":"ad1a423b-0aac-4db1-cc32-88dea39d3600","executionInfo":{"status":"ok","timestamp":1556969241096,"user_tz":-120,"elapsed":628,"user":{"displayName":"Amal Abdelsalam","photoUrl":"https://lh6.googleusercontent.com/-MsHyBKvIoWc/AAAAAAAAAAI/AAAAAAAAAGk/D_9qsqnwXZE/s64/photo.jpg","userId":"16383286328372397172"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["print(\"State space: \", env.observation_space)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["State space:  Discrete(500)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"v55A6-dYYpMy","colab_type":"text"},"source":["#### And what actions are available?"]},{"cell_type":"code","metadata":{"id":"XPuU9DQdYpM1","colab_type":"code","outputId":"1dd133a7-ac4b-4306-eae7-89938a0a8611","executionInfo":{"status":"ok","timestamp":1556969244503,"user_tz":-120,"elapsed":808,"user":{"displayName":"Amal Abdelsalam","photoUrl":"https://lh6.googleusercontent.com/-MsHyBKvIoWc/AAAAAAAAAAI/AAAAAAAAAGk/D_9qsqnwXZE/s64/photo.jpg","userId":"16383286328372397172"}},"colab":{"base_uri":"https://localhost:8080/","height":35}},"source":["print(\"Action space: \", env.action_space)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Action space:  Discrete(6)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ZjvYU2t_YpM_","colab_type":"text"},"source":["#### Your task: Intialize Q table and Implement the Q update function\n","In the following code we have a simple implementation of the QLearning algorithm where you are required to implement its Q update function.\n","Make sure you understand the flow of the algorithm very well as it will be the basis for all upcoming exercises as well.\n","\n","Note: no exploration considered in this implementation."]},{"cell_type":"code","metadata":{"id":"b7BmAuLpYpNB","colab_type":"code","colab":{}},"source":["def QLearning(env, alpha, gamma, epsilon, episodes):\n","    \n","    # initialize Q values\n","    Q = # < Add your code here>  \n","\n","    all_epochs = []\n","    all_penalties = []\n","    frames = []\n","    for i in range(1, episodes):\n","        state = env.reset()\n","        \n","        # initialize variables\n","        epochs, penalties, reward, = 0, 0, 0\n","        done = False\n","\n","        while not done:\n","            action = np.argmax(Q[state])\n","\n","            next_state, reward, done, info = env.step(action)\n","\n","            # Put each rendered frame into dict for animation\n","            frames.append({\n","                'frame': env.render(mode='ansi'),\n","                'state': next_state,\n","                'action': action,\n","                'reward': reward\n","                }\n","            )\n","            \n","            old_value = Q[state, action]\n","            next_max = np.max(Q[next_state])\n","\n","            # Update q-table\n","            # < Add your code here>  \n","            \n","            \n","            if reward == -10:\n","                penalties += 1\n","\n","            state = next_state\n","            epochs += 1\n","\n","        if i % 100 == 0:\n","            clear_output(wait=True)\n","            print(\"Episode: %s\" % i)\n","            env.render()\n","    return frames\n","\n","import time\n","def print_frames(frames):\n","    for i, frame in enumerate(frames):\n","        clear_output(wait=True)\n","        print(frame['frame'])\n","        print(\"Timestep: %d\" % (i + 1))\n","        print(\"State: %s\" % frame['state'])\n","        print(\"Action: %s\" % frame['action'])\n","        print(\"Reward: %s\" % frame['reward'])\n","        time.sleep(.1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZpefFpAuYpNH","colab_type":"text"},"source":["#### Now let's initialize the Q-Learning parameters and see the algorithm in action."]},{"cell_type":"code","metadata":{"id":"INd-e2oQYpNJ","colab_type":"code","colab":{}},"source":["# Hyperparameters intialization\n","alpha = 0.1\n","gamma = 0.6\n","epsilon = 0.1\n","episods = 100001\n","            \n","frames = QLearning(env, alpha, gamma, epsilon, episods)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CFfoaDmTYpNQ","colab_type":"text"},"source":["#### Let's see how agent's actions evolve over the multiple episodes\n","printing steps in slow motion"]},{"cell_type":"code","metadata":{"id":"YpxRDl_lYpNS","colab_type":"code","colab":{}},"source":["# print latest episodes to see if the agent was able to learn anything\n","print_frames(frames[-200:])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"myaFRPLz_pS_","colab_type":"code","colab":{}},"source":["env.close()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3WJf3ZewYpNb","colab_type":"text"},"source":["## Second Exercise : Mountain Car\n","\n","A car is on a one-dimensional track, positioned between two mountains. The goal is to drive up the mountain on the right; however, the car's engine is not strong enough to scale the mountain in a single pass. Therefore, the only way to succeed is to drive back and forth to build up momentum.\n","\n","Moutain Car, on the contrary to the Taxi, have states in a continuous space (box space). The States of the Car are represented with 2 continuous elements\n","\n","1.   car position\n","2.   car velocity\n","\n","However the action of the mountain car is discrete\n","\n","0: push left, \n","1: no push, and \n","2: push right"]},{"cell_type":"code","metadata":{"id":"-LCleFKPYpNd","colab_type":"code","outputId":"fd388ef9-4a53-4e65-9eac-b34c834ed910","executionInfo":{"status":"ok","timestamp":1556969345799,"user_tz":-120,"elapsed":827,"user":{"displayName":"Amal Abdelsalam","photoUrl":"https://lh6.googleusercontent.com/-MsHyBKvIoWc/AAAAAAAAAAI/AAAAAAAAAGk/D_9qsqnwXZE/s64/photo.jpg","userId":"16383286328372397172"}},"colab":{"base_uri":"https://localhost:8080/","height":87}},"source":["import gym\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","env = gym.make('MountainCar-v0')\n","\n","print(\"State space: \", env.observation_space)\n","print(\"low: %s \\n high: %s\" %(env.observation_space.low, env.observation_space.high))\n","print(\"Action space: \", env.action_space)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["State space:  Box(2,)\n","low: [-1.2  -0.07] \n"," high: [0.6  0.07]\n","Action space:  Discrete(3)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"uvzInozOYpN2","colab_type":"text"},"source":["#### Your tasks:  \n","1.   implement the greedy epsilon strategy. \n","2.   reuse pre-implemented Q update rule, from previous exercise.\n","3.   track and plot the change in reward values along training episodes, e.g. plot the average reward values at each x episodes.\n"]},{"cell_type":"code","metadata":{"id":"5vuAJN0kYpN4","colab_type":"code","colab":{}},"source":["def QLearning(env, alpha, gamma, epsilon, min_eps, episodes):\n","    \n","    # Determine size of discretized state space\n","    num_states = (env.observation_space.high - env.observation_space.low)*\\\n","                    np.array([10, 100])\n","    num_states = np.round(num_states, 0).astype(int) + 1\n","    \n","    # Initialize q-table\n","    Q = np.random.uniform(low = -1, high = 1, \n","                          size = (num_states[0], num_states[1], \n","                                  env.action_space.n))\n","    \n","    # Initialize variables to track rewards\n","    reward_list = []\n","    ave_reward_list = []\n","    \n","    # Calculate episodic reduction in epsilon\n","    reduction = (epsilon - min_eps)/episodes\n","    \n","    # Run Q learning algorithm\n","    for i in range(episodes):\n","        # Initialize parameters\n","        done = False\n","        tot_reward, reward = 0,0\n","        state = env.reset()\n","        \n","        # Discretize state\n","        state_adj = (state - env.observation_space.low) * np.array([10, 100])\n","        state_adj = np.round(state_adj, 0).astype(int)\n","    \n","        while done != True:   \n","            # Render environment for last five episodes\n","            if i >= (episodes - 20):\n","                env.render()\n","                \n","            # Determine next action - epsilon greedy strategy\n","            # < Add your code here>\n","            \n","            \n","                \n","            # Get next state and reward\n","            # < Add your code here>\n","\n","            \n","            # Discretize next state\n","            next_state_adj = (next_state - env.observation_space.low) * np.array([10, 100])\n","            next_state_adj = np.round(next_state_adj, 0).astype(int)\n","            \n","            # Allow for terminal states\n","            if done and next_state[0] >= 0.5:\n","                Q[state_adj[0], state_adj[1], action] = reward\n","                \n","            # Adjust Q value for current state\n","            else:\n","                \n","                # Reuse your implementation for the Q update function in the previous example\n","                # < Add your code here>\n","                \n","               \n","            \n","            # Update variables\n","            tot_reward += reward\n","            state_adj = next_state_adj\n","        \n","        # Decay epsilon\n","        if epsilon > min_eps:\n","            epsilon -= reduction\n","        \n","        # Track rewards\n","        reward_list.append(tot_reward)\n","        \n","        if (i+1) % 100 == 0:\n","            ave_reward = np.mean(reward_list)\n","            ave_reward_list.append(ave_reward)\n","            reward_list = []\n","   \n","        if (i+1) % 100 == 0:    \n","            print('Episode {} Average Reward: {}'.format(i+1, ave_reward))\n","            env.render()     \n","    \n","    return ave_reward_list"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_fI-IgLVYpN_","colab_type":"code","colab":{}},"source":["#run the Qlearning to train our agent, this will take a bit of time, especially the rendering part\n","env.reset()\n","\n","# initialize hyperparameters then train the agent ..\n","\n","# note: epsilon is used as a parameter for exploration\n","# but as we further go with the training the system actually needs to exploit\n","# it's trained model so we reduce the epsilon with the episodes\n","# it follows a linear schedule starting from epsilon to min_eps\n","# over the episodes\n","# epsilon and min eps are probabilites this should give you an intuition about the range\n","# gammwe is better used in range of 0.7-0.99 as a gamma of 1 loses the previliage of discount\n","# alpha is the learning rate, use in range of 0.15-0.5\n","# finally you have to set the episode number as the more number of episodes the better\n","# use # of episodes in range 30,000 to 70,000\n","# < Add your code here>\n","\n","\n","\n","rewards = "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"O9-3KvwYYpOH","colab_type":"code","colab":{}},"source":["# close rendering window\n","env.close()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sv8gmemQYpOT","colab_type":"code","colab":{}},"source":["# Plot Rewards vs Episodes\n","# Note: you will need to modify the QLearning function to keep track with the reward value over each episode \n","\n","# < Add your code here>\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1UIoyu7UYpOb","colab_type":"text"},"source":["## Third Exercise: DQN\n","\n","In this exercise, we are going to practice implementing DQN for atari game learning, Pong in this exercise. But before we dig into the details of the DQN model, let's first apply few pre-processing steps on our data (game frames).\n","\n","For preprocessing , we are going to use OpenAI Gym Wrappers. These wrappers make it easier to interact with OpenAI Gym. \n","\n","Starting simple, we are going to apply only two wrappers\n","\n","1.   frame processing (downsampling and greyscaling)\n","2.   image normalization\n","\n","as you can notice in the make_env() function below. (which we are going to use to create our environment later)"]},{"cell_type":"code","metadata":{"id":"84kstlQTSfwm","colab_type":"code","colab":{}},"source":["import cv2\n","import gym\n","import random\n","import numpy as np\n","from itertools import combinations\n","from collections import deque\n","\n","# Taken from OpenAI baseline wrappers\n","# https://github.com/openai/baselines/blob/master/baselines/common/atari_wrappers.py\n","\n","\n","class ProcessFrame84(gym.ObservationWrapper):\n","    \"\"\"\n","    Downsamples image to 84x84\n","    Greyscales image\n","\n","    Returns numpy array\n","    \"\"\"\n","    def __init__(self, env=None):\n","        super(ProcessFrame84, self).__init__(env)\n","        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n","\n","    def observation(self, obs):\n","        return ProcessFrame84.process(obs)\n","\n","    @staticmethod\n","    def process(frame):\n","        if frame.size == 210 * 160 * 3:\n","            img = np.reshape(frame, [210, 160, 3]).astype(np.float32)\n","        elif frame.size == 250 * 160 * 3:\n","            img = np.reshape(frame, [250, 160, 3]).astype(np.float32)\n","        else:\n","            assert False, \"Unknown resolution.\"\n","        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n","        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n","        x_t = resized_screen[18:102, :]\n","        x_t = np.reshape(x_t, [84, 84, 1])\n","        return x_t.astype(np.uint8)\n","\n","    \n","class ScaledFloatFrame(gym.ObservationWrapper):\n","    \"\"\"Normalize pixel values in frame --> 0 to 1\"\"\"\n","    def observation(self, obs):\n","        return np.array(obs).astype(np.float32) / 255.0\n","\n","def make_env(env_name):\n","    env = gym.make(env_name)\n","    env = ProcessFrame84(env)\n","    return ScaledFloatFrame(env)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dMI8go1EbUu9","colab_type":"text"},"source":["### Time to build our DQN approximator"]},{"cell_type":"code","metadata":{"id":"FwB45OJwbWPj","colab_type":"code","colab":{}},"source":["from keras.models import Sequential\n","from keras.layers import Dense\n","from keras.optimizers import Adam\n","\n","class DQNAgent:\n","    \n","    def __init__(self, state_size, action_size):\n","        self.state_size = state_size\n","        self.action_size = action_size\n","        self.memory = deque(maxlen=100000)\n","        self.gamma = 0.99    \n","        self.epsilon = 1.0\n","        self.epsilon_min = 0.02\n","        self.epsilon_decay = 0.995\n","        self.learning_rate = 0.0001\n","        self.model = self._build_model()\n","\n","    def _build_model(self):\n","        \n","        # This function should return a model that learn the Q-function\n","        # that can be further trained \n","        # when desired, in other words you should build your computational graph\n","        # based on your input shape and the number of the actions\n","        # it's preferable to use keras\n","        \n","        # < Add your code here>\n","        \n","        \n","        return model\n","\n","    def remember(self, state, action, reward, next_state, done):\n","        self.memory.append((state, action, reward, next_state, done))\n","\n","    #get action\n","    def act(self, state):\n","        # select random action with prob=epsilon else action=maxQ\n","        if np.random.rand() <= self.epsilon:\n","            return random.randrange(self.action_size)\n","        \n","        # you have your model and your state now you should \n","        # get action value\n","        # < Add your code here>\n","         \n","        \n","        return np.argmax(act_values[0])  # returns action\n","\n","    def replay(self, batch_size):\n","        # experience replay\n","        # sample random transitions\n","        minibatch = random.sample(self.memory, batch_size)\n","\n","        for state, action, reward, next_state, done in minibatch:\n","            target = reward\n","\n","            if not done:\n","                #calculate target for each minibatch\n","                Q_next=self.model.predict(next_state)[0]  # Q=NN.predict(state)\n","                target = (reward + self.gamma *np.amax(Q_next)) #Belman\n","\n","            target_f = self.model.predict(state)\n","            target_f[0][action] = target\n","\n","            #train network\n","            self.model.fit(state, target_f, epochs=1, verbose=0)\n","\n","        if self.epsilon > self.epsilon_min:\n","            self.epsilon *= self.epsilon_decay\n","        \n","    def learn(self, env, episodes, batch_size):\n","        \n","        for e in range(episodes):\n","            state = env.reset()\n","            state = np.reshape(state, [1, state_size])\n","            done = False\n","            \n","            total_reward = 0\n","            while not done:\n","                # epsilon-greedy action\n","                action = self.act(state)\n","\n","                next_state, reward, done, _ = env.step(action)\n","\n","                total_reward += reward \n","                \n","                next_state = np.reshape(next_state, [1, state_size])\n","\n","                #add to experience memory\n","                self.remember(state, action, reward, next_state, done)\n","\n","                state = next_state\n","\n","                if done:\n","                    print(\"episode: {}/{}, reward: {}\"\n","                          .format(e, episodes, total_reward))\n","                    break\n","                    \n","            #experience replay\n","            if len(self.memory) > batch_size:\n","                self.replay(batch_size)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-vIAu6dAbibZ","colab_type":"text"},"source":["All together"]},{"cell_type":"code","metadata":{"id":"zpusFr41aVL3","colab_type":"code","colab":{}},"source":["import gym\n","import random\n","import numpy as np\n","from itertools import combinations \n","\n","env = make_env('PongNoFrameskip-v4')\n","\n","env.reset()\n","state_size = np.prod(env.observation_space.shape)\n","action_size = env.action_space.n\n","agent = DQNAgent(state_size, action_size)\n","print(state_size)\n","\n","batch_size = 32\n","episodes = 500\n","\n","agent.learn(env, episodes, batch_size)\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"msmXCQiqU37B","colab_type":"text"},"source":["### Finally, let's see how our trained agent could act like if it's trained for longer time. Say, 5 millions time steps!\n"]},{"cell_type":"code","metadata":{"id":"Z5xKZaoEWWc8","colab_type":"code","colab":{}},"source":["!python -m baselines.run --alg=ppo2 --env=PongNoFrameskip-v4 --num_timesteps=0 --load_path=pong_5M_ppo2 --play"],"execution_count":0,"outputs":[]}]}